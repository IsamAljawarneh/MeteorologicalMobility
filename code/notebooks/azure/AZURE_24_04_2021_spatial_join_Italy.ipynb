{"nbformat_minor": 2, "cells": [{"execution_count": null, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,harsha2010:magellan:1.0.5-s_2.11,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0,org.apache.spark:spark-sql_2.11:2.2.0\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\",\n        \"spark.dynamicAllocation.enabled\": false\n    }\n}", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 16.784912109375, "end_time": 1619314077485.055}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "/**\n * @Description: Joining Italy mobility data (semi-synthetic) with neighborhoods (polygons)\n * N.B. : part of MeteoMobility project\n * @author: Isam Al Jawarneh\n * @date: 02/02/2019\n * @last update: 25/04/2021\n */", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 749.881103515625, "end_time": 1619314108053.924}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "import util.control.Breaks._\nimport org.apache.spark.sql.streaming.StreamingQueryListener\nimport org.apache.spark.util.random.XORShiftRandom\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.types._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SQLImplicits\nimport org.apache.spark.sql.functions.from_json\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.sql.ForeachWriter\nimport magellan._\nimport magellan.index.ZOrderCurve\nimport magellan.{Point, Polygon}\n\nimport org.apache.spark.sql.magellan.dsl.expressions._\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.streaming.OutputMode\nimport org.apache.spark.sql.types.{\n  DoubleType,\n  StringType,\n  StructField,\n  StructType\n}\nimport org.apache.spark.sql.streaming._\nimport org.apache.spark.sql.streaming.Trigger\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nimport org.apache.spark.sql.functions.{collect_list, collect_set}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.log4j.{Level, Logger}\nimport scala.collection.mutable\nimport scala.concurrent.duration.Duration\nimport java.io.{BufferedWriter, FileWriter}\nimport org.apache.commons.io.FileUtils\nimport java.io.File\nimport scala.collection.mutable.ListBuffer\nimport java.time.Instant\nimport org.apache.spark.util.CollectionAccumulator", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 7302.81298828125, "end_time": 1619314115368.228}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "var precision = 30\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 742.575927734375, "end_time": 1619314116120.506}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "/*\nhere attention, longitude, latitude!\n*/\nval schemaBO = StructType(Array(\nStructField(\"id\", LongType, false),\nStructField(\"data_id\", IntegerType, false),\n    StructField(\"received_timestamp\", StringType, false),\n    StructField(\"accuracy\", DoubleType, false),\n    StructField(\"latitude\", DoubleType, false),\n    StructField(\"longitude\", DoubleType, false),\n    StructField(\"provider\", StringType, false),\n    StructField(\"user_id\", IntegerType, false),\n    StructField(\"sample_timestamp\", StringType, false)))", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1258.908935546875, "end_time": 1619314117389.63}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "val geohashUDF = udf{(curve: Seq[ZOrderCurve]) => curve.map(_.toBase32())}", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2263.35498046875, "end_time": 1619314119662.137}}, "collapsed": false}}, {"source": "## we first explode the mobility data into the corresponding geohashes\n***\n**\npreparing it thus for the spatial join job, by utilizing the filter-refine (a.k.a. true-hit filtering) approach\n**\n***", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val points = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schemaBO).csv(\"wasb://sspark@7q6kgdctotuwu.blob.core.windows.net/bo/dat/\").withColumn(\"point\", point($\"longitude\",$\"latitude\")).select(\"point\",\"id\")//.limit(20000)\nval ridesGeohashed1 = points.withColumn(\"index\", $\"point\" index  precision).withColumn(\"geohashArray\", geohashUDF($\"index.curve\"))\nval explodedRidesGeohashed1 = ridesGeohashed1.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 5283.484130859375, "end_time": 1619314124955.461}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "explodedRidesGeohashed1.show(2)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 5283.150146484375, "end_time": 1619314130247.578}}, "collapsed": false}}, {"source": "## we then explode Bologna geojson neighborhood file\n***\n**\nto get the geohash covering of Bologna\n**\n***", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "//bologna geojson\nval rawNeighborhoodsBO = spark.sqlContext.read.format(\"magellan\").option(\"type\", \"geojson\").load(\"wasb://sspark@7q6kgdctotuwu.blob.core.windows.net/bo/bolognaQuartiere\").select($\"polygon\", $\"metadata\"(\"NOMEQUART\").as(\"neighborhood\"))\n\nval neighborhoodsBO = rawNeighborhoodsBO.withColumn(\"index\", $\"polygon\" index precision).select($\"polygon\", $\"index\", \n      $\"neighborhood\")\n\nval zorderIndexedNeighborhoodsBO = neighborhoodsBO.withColumn(\"index\", explode($\"index\")).select(\"polygon\", \"index.curve\", \"index.relation\",\"neighborhood\")\nval geohashedNeighborhoodsBO= neighborhoodsBO.withColumn(\"geohashArray\", geohashUDF($\"index.curve\"))\n\nval explodedgeohashedNeighborhoodsBO = geohashedNeighborhoodsBO.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\n\n/////////////////////////\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2261.97900390625, "end_time": 1619314132519.973}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "explodedgeohashedNeighborhoodsBO.count()", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 5275.675048828125, "end_time": 1619314137805.046}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "explodedgeohashedNeighborhoodsBO.select(\"*\").where(explodedgeohashedNeighborhoodsBO(\"geohash\")===\"spzvpt\").show()", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2261.34912109375, "end_time": 1619314140076.167}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "explodedgeohashedNeighborhoodsBO.show(5)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 3279.996826171875, "end_time": 1619314143366.573}}, "collapsed": false}}, {"source": "## we then explode Italy geojson neighborhood file\n***\n**\nto get the geohash covering of Italy\n**\n***", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "//italy geojson\nval rawNeighborhoods = spark.sqlContext.read.format(\"magellan\").option(\"type\", \"geojson\").load(\"wasb://sspark@7q6kgdctotuwu.blob.core.windows.net/bo/italy/\").select($\"polygon\", $\"metadata\"(\"name\").as(\"neighborhood\")).cache()\n\nval neighborhoods = rawNeighborhoods.withColumn(\"index\", $\"polygon\" index  precision).select($\"polygon\", $\"index\", \n      $\"neighborhood\").cache()\n\nval zorderIndexedNeighborhoods = neighborhoods.withColumn(\"index\", explode($\"index\")).select(\"polygon\", \"index.curve\", \"index.relation\",\"neighborhood\")\nval geohashedNeighborhoods= neighborhoods.withColumn(\"geohashArray\", geohashUDF($\"index.curve\"))\n\nval explodedgeohashedNeighborhoods = geohashedNeighborhoods.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\n\n\n/////////////////////////\n\n\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2260.575927734375, "end_time": 1619314145638.453}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "explodedgeohashedNeighborhoods.show(1)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 35424.3291015625, "end_time": 1619314181071.998}}, "collapsed": false}}, {"source": "## we then exclude bologna covering from the geohash covery of Italy neighborhoods\n***\n**\nwe do so because the mobility data are not all in Bologna\nthe geojson file of Bologna contains all the neighborhoods\nwhile the geojson file of Italy contains only cities (covering of cities)\n**\n***", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val newexplodedgeohashedNeighborhoods = explodedgeohashedNeighborhoods.filter(col(\"neighborhood\") =!= \"Bologna\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 743.93896484375, "end_time": 1619314181826.185}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "explodedgeohashedNeighborhoods.count()", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1251.005859375, "end_time": 1619314183086.861}}, "collapsed": false}}, {"source": "## we union both neighborhood data\n***\n**\nto get the geohash covering of all Italy, including bologna on a granular level (neigborhoods of Bologna == 40)\n**\n***", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val allNeigborhoods = newexplodedgeohashedNeighborhoods.union(explodedgeohashedNeighborhoodsBO)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 743.881103515625, "end_time": 1619314183839.933}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "allNeigborhoods.count()", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2258.301025390625, "end_time": 1619314186108.089}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "explodedgeohashedNeighborhoods.show(2)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 760.385009765625, "end_time": 1619314186877.496}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "explodedgeohashedNeighborhoods.select(\"*\").where(explodedgeohashedNeighborhoods(\"geohash\")===\"spzvpt\").show()", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 3280.762939453125, "end_time": 1619314190168.422}}, "collapsed": false}}, {"source": "## now we perform the spatial join\n***\n**\nusing filter-refine approach:\nfirst: filter --> performing the MBR-join, using the geohash paired with geohash covering, a cheap operation\nsecond: the refine step, where we ensure wether candidate tuples fall in real geometries within a specific neighborhood or not!\n**\n***", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val rawTripsJoinedBSO = explodedRidesGeohashed1.join(allNeigborhoods, explodedRidesGeohashed1(\"geohash\") === allNeigborhoods(\"geohash\")).select(\"point\", \"neighborhood\",\"id\").where($\"point\" within $\"polygon\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 743.02099609375, "end_time": 1619314190921.919}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "rawTripsJoinedBSO.count()", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 13314.606201171875, "end_time": 1619314204245.192}}, "collapsed": false}}, {"source": "## we do aggregations\n***\n**\nTop-K spatial query to check which neighborhoods have more dense mobility data\n**\n***", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "rawTripsJoinedBSO.select(\"*\").groupBy(col(\"neighborhood\")).agg(count(\"*\").as(\"count\")).sort(desc(\"count\")).show(20)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 13332.80712890625, "end_time": 1619314217589.126}}, "collapsed": false}}, {"source": "## DONE!", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}